"""
Modelo Refinado con VIX Contextual - Por Categor√≠a
Optimiza Œ± y Œ≤ espec√≠ficos para CADA tipo de noticia

HIP√ìTESIS:
- Noticias de guerra/terror: Œ≤ alto (efecto polvor√≠n extremo)
- Noticias Fed/ECB: Œ± y Œ≤ moderados
- Noticias housing/earnings: Œ± bajo (poco efecto VIX)
"""
import pandas as pd
import numpy as np
from datetime import datetime
import pickle
import sys
from pathlib import Path
import json

sys.path.append(str(Path(__file__).resolve().parent.parent.parent))

from src.utils.config import RAW_DATA_DIR, PROCESSED_DATA_DIR, MODELS_DIR
from src.utils.logger import logger


class ModeloRefinadoVIXCategorias:
    """
    Modelo que optimiza Œ± y Œ≤ por categor√≠a de noticia
    """
    
    def __init__(self):
        # Par√°metros por categor√≠a
        self.params_por_categoria = {}
        
        # VIX cr√≠tico
        self.vix_critico = 20.0
        
        # Datos
        self.df_tokens = None
        self.df_noticias = None
        self.df_historico = None
        
        logger.info("‚úì ModeloRefinadoVIXCategorias inicializado")
    
    def cargar_datos(self):
        """Carga todos los datos necesarios"""
        logger.info("\n" + "="*70)
        logger.info("CARGANDO DATOS")
        logger.info("="*70)
        
        # Tokens
        token_files = list(PROCESSED_DATA_DIR.glob("landau/tokens_volatilidad_*.csv"))
        if token_files:
            self.df_tokens = pd.read_csv(token_files[0])
            logger.info(f"  ‚úì Tokens: {len(self.df_tokens)} combinaciones")
        
        # Hist√≥rico
        hist_files = list(PROCESSED_DATA_DIR.glob("landau/parametros_landau_historicos_*.csv"))
        if hist_files:
            self.df_historico = pd.read_csv(hist_files[0])
            self.df_historico['fecha'] = pd.to_datetime(self.df_historico['fecha'])
            self.df_historico['fecha_norm'] = pd.to_datetime([str(d).split()[0] for d in self.df_historico['fecha']])
            logger.info(f"  ‚úì Hist√≥rico: {len(self.df_historico)} d√≠as")
        
        # Noticias
        kaggle_dir = RAW_DATA_DIR / "Kanggle"
        df_combined = pd.read_csv(kaggle_dir / "Combined_News_DJIA.csv")
        df_combined['Date'] = pd.to_datetime(df_combined['Date'])
        
        noticias = []
        for idx, row in df_combined.iterrows():
            for i in range(1, 26):
                noticia = row.get(f'Top{i}')
                if pd.notna(noticia) and noticia != '':
                    noticias.append({
                        'fecha': row['Date'],
                        'titulo': noticia
                    })
        
        self.df_noticias = pd.DataFrame(noticias)
        logger.info(f"  ‚úì Noticias: {len(self.df_noticias)}")
        
        self.clasificar_noticias_detallado()
        
        return True
    
    def clasificar_noticias_detallado(self):
        """Clasificaci√≥n detallada con m√∫ltiples categor√≠as"""
        categorias = {
            # ALTA VOLATILIDAD (esperamos Œ≤ alto)
            'terrorism': ['terror', 'bombing', 'attack', 'killed'],
            'war_russia': ['russia', 'ukraine', 'putin', 'kremlin'],
            'war_middle_east': ['iran', 'iraq', 'syria', 'israel', 'palestine'],
            'financial_crisis': ['crisis', 'crash', 'panic', 'bailout', 'collapse'],
            
            # MEDIA-ALTA VOLATILIDAD (esperamos Œ± y Œ≤ moderado-alto)
            'fed_rates': ['fed', 'fomc', 'interest rate', 'federal reserve'],
            'ecb_policy': ['ecb', 'draghi', 'lagarde', 'european central bank'],
            'oil_shock': ['oil price', 'opec', 'crude', 'petroleum'],
            
            # MEDIA VOLATILIDAD (esperamos Œ± y Œ≤ moderado)
            'us_gdp_data': ['gdp', 'economic growth', 'gross domestic'],
            'us_employment_data': ['employment', 'jobs', 'unemployment', 'payroll'],
            'china_economy': ['china', 'beijing', 'yuan', 'chinese'],
            
            # BAJA VOLATILIDAD (esperamos Œ± bajo, Œ≤ bajo)
            'us_housing': ['housing', 'home sales', 'real estate'],
            'corporate_earnings': ['earnings', 'profit', 'quarterly results'],
            'trade_data': ['trade', 'exports', 'imports'],
        }
        
        self.df_noticias['categoria'] = 'other'
        
        for idx, row in self.df_noticias.iterrows():
            titulo = str(row['titulo']).lower()
            for cat, keywords in categorias.items():
                if any(kw in titulo for kw in keywords):
                    self.df_noticias.at[idx, 'categoria'] = cat
                    break
        
        # Estad√≠sticas
        logger.info("\n  Distribuci√≥n por categor√≠a:")
        for cat in self.df_noticias['categoria'].value_counts().head(10).items():
            logger.info(f"    {cat[0]:<25s}: {cat[1]:>5,} noticias")
    
    def preparar_datos_por_categoria(self):
        """
        Prepara datasets separados por categor√≠a
        """
        logger.info("\n" + "="*70)
        logger.info("PREPARANDO DATOS POR CATEGOR√çA")
        logger.info("="*70)
        
        datos_por_categoria = {}
        
        categorias_validas = [c for c in self.df_noticias['categoria'].unique() if c != 'other']
        
        for categoria in categorias_validas:
            logger.info(f"\n  Procesando: {categoria}")
            
            df_cat = self.df_noticias[self.df_noticias['categoria'] == categoria]
            
            datos_opt = []
            
            # Buscar token
            token_data = self.df_tokens[
                (self.df_tokens['categoria'] == categoria) & 
                (self.df_tokens['asset'] == 'SPY')
            ]
            
            if len(token_data) == 0:
                logger.info(f"    ‚ö† Sin token, saltando")
                continue
            
            token = token_data.iloc[0]['token']
            p_base = (token / 10.0) * 100
            
            # Para cada noticia de esta categor√≠a
            for idx, row in df_cat.iterrows():
                fecha = pd.Timestamp(row['fecha']).normalize()
                
                # Buscar VIX y retorno
                hist_data = self.df_historico[self.df_historico['fecha_norm'] == fecha]
                
                if len(hist_data) == 0:
                    # Buscar en ventana
                    for offset in range(1, 4):
                        fecha_alt = fecha + pd.Timedelta(days=offset)
                        hist_data = self.df_historico[self.df_historico['fecha_norm'] == fecha_alt]
                        if len(hist_data) > 0:
                            break
                
                if len(hist_data) > 0:
                    vix_dia = hist_data.iloc[0]['vix']
                    retorno_real = hist_data.iloc[0]['sp500_return_1d']
                    
                    if pd.notna(vix_dia) and pd.notna(retorno_real):
                        impacto_real = 1 if abs(retorno_real) > 0.005 else 0
                        
                        datos_opt.append({
                            'p_base': p_base,
                            'vix': float(vix_dia),
                            'impacto_real': impacto_real,
                            'retorno_real': float(retorno_real)
                        })
            
            if len(datos_opt) >= 30:  # M√≠nimo 30 observaciones
                df_opt = pd.DataFrame(datos_opt)
                datos_por_categoria[categoria] = df_opt
                
                logger.info(f"    ‚úì {len(df_opt)} observaciones")
                logger.info(f"      Impacto (>0.5%): {df_opt['impacto_real'].mean()*100:.1f}%")
                logger.info(f"      VIX promedio: {df_opt['vix'].mean():.1f}")
            else:
                logger.info(f"    ‚ö† Solo {len(datos_opt)} obs, insuficiente")
        
        logger.info(f"\n‚úì Total categor√≠as con datos: {len(datos_por_categoria)}")
        
        return datos_por_categoria
    
    def calcular_impacto_contextual(self, p_base, vix, alpha, beta):
        """Calcula impacto con par√°metros espec√≠ficos"""
        v_norm = vix / self.vix_critico
        
        if v_norm <= 1.0:
            impacto = p_base * (1.0 - alpha * 0.1 * (1 - v_norm))
        else:
            impacto = p_base * (1.0 + alpha * ((v_norm - 1.0) ** beta))
        
        return max(0, min(100, impacto))
    
    def optimizar_categoria(self, categoria, df_cat):
        """
        Optimiza Œ± y Œ≤ para UNA categor√≠a espec√≠fica
        """
        logger.info(f"\n  Optimizando: {categoria}")
        
        try:
            from skopt import gp_minimize
            from skopt.space import Real
            from sklearn.metrics import f1_score
        except:
            logger.warning("    ‚ö† scikit-optimize no instalado, usando grid search")
            return self.optimizar_categoria_grid(categoria, df_cat)
        
        # Funci√≥n objetivo
        def objetivo(params):
            alpha, beta = params
            
            predictions = []
            for _, row in df_cat.iterrows():
                p_ctx = self.calcular_impacto_contextual(
                    row['p_base'],
                    row['vix'],
                    alpha,
                    beta
                )
                predictions.append(1 if p_ctx > 50 else 0)
            
            predictions = np.array(predictions)
            real = df_cat['impacto_real'].values
            
            # F1-score
            try:
                f1 = f1_score(real, predictions)
            except:
                f1 = 0.5
            
            return -f1
        
        # Espacio de b√∫squeda
        space = [
            Real(0.1, 2.5, name='alpha'),
            Real(0.5, 3.0, name='beta'),
        ]
        
        # Optimizar
        resultado = gp_minimize(
            objetivo,
            space,
            n_calls=30,  # 30 iteraciones por categor√≠a
            random_state=42,
            verbose=False
        )
        
        alpha_opt = resultado.x[0]
        beta_opt = resultado.x[1]
        f1_opt = -resultado.fun
        
        logger.info(f"    ‚úì Œ±={alpha_opt:.3f}, Œ≤={beta_opt:.3f}, F1={f1_opt:.3f}")
        
        return {
            'alpha': alpha_opt,
            'beta': beta_opt,
            'f1_score': f1_opt,
            'n_obs': len(df_cat)
        }
    
    def optimizar_categoria_grid(self, categoria, df_cat):
        """Grid search como fallback"""
        from sklearn.metrics import f1_score
        
        mejor_f1 = 0
        mejor_params = {'alpha': 0.75, 'beta': 1.5}
        
        for alpha in [0.3, 0.5, 0.75, 1.0, 1.5]:
            for beta in [0.8, 1.0, 1.2, 1.5, 2.0]:
                predictions = []
                for _, row in df_cat.iterrows():
                    p_ctx = self.calcular_impacto_contextual(
                        row['p_base'],
                        row['vix'],
                        alpha,
                        beta
                    )
                    predictions.append(1 if p_ctx > 50 else 0)
                
                predictions = np.array(predictions)
                real = df_cat['impacto_real'].values
                
                try:
                    f1 = f1_score(real, predictions)
                    if f1 > mejor_f1:
                        mejor_f1 = f1
                        mejor_params = {'alpha': alpha, 'beta': beta}
                except:
                    pass
        
        logger.info(f"    ‚úì Œ±={mejor_params['alpha']:.3f}, Œ≤={mejor_params['beta']:.3f}, F1={mejor_f1:.3f}")
        
        return {
            'alpha': mejor_params['alpha'],
            'beta': mejor_params['beta'],
            'f1_score': mejor_f1,
            'n_obs': len(df_cat)
        }
    
    def optimizar_todas_categorias(self, datos_por_categoria):
        """
        Optimiza Œ± y Œ≤ para todas las categor√≠as
        """
        logger.info("\n" + "="*70)
        logger.info("OPTIMIZACI√ìN BAYESIANA POR CATEGOR√çA")
        logger.info("="*70)
        
        for categoria, df_cat in datos_por_categoria.items():
            params = self.optimizar_categoria(categoria, df_cat)
            self.params_por_categoria[categoria] = params
        
        logger.info("\n‚úì Optimizaci√≥n completada")
        logger.info(f"  Total categor√≠as: {len(self.params_por_categoria)}")
    
    def analizar_parametros(self):
        """
        Analiza los par√°metros encontrados por tipo de noticia
        """
        logger.info("\n" + "="*70)
        logger.info("AN√ÅLISIS DE PAR√ÅMETROS POR CATEGOR√çA")
        logger.info("="*70)
        
        # Ordenar por Œ≤ (efecto polvor√≠n)
        categorias_sorted = sorted(
            self.params_por_categoria.items(),
            key=lambda x: x[1]['beta'],
            reverse=True
        )
        
        logger.info("\nüî• RANKING POR EFECTO POLVOR√çN (Œ≤):")
        logger.info("‚îÄ"*70)
        logger.info(f"{'Categor√≠a':<25s} | {'Œ±':>6s} | {'Œ≤':>6s} | {'F1':>6s} | {'Obs':>6s}")
        logger.info("‚îÄ"*70)
        
        for cat, params in categorias_sorted:
            logger.info(f"{cat:<25s} | {params['alpha']:>6.3f} | {params['beta']:>6.3f} | "
                       f"{params['f1_score']:>6.3f} | {params['n_obs']:>6,}")
        
        # Estad√≠sticas
        betas = [p['beta'] for p in self.params_por_categoria.values()]
        alphas = [p['alpha'] for p in self.params_por_categoria.values()]
        
        logger.info("\nüìä ESTAD√çSTICAS:")
        logger.info(f"  Œ≤ promedio: {np.mean(betas):.3f}")
        logger.info(f"  Œ≤ min-max: [{np.min(betas):.3f}, {np.max(betas):.3f}]")
        logger.info(f"  Œ± promedio: {np.mean(alphas):.3f}")
        logger.info(f"  Œ± min-max: [{np.min(alphas):.3f}, {np.max(alphas):.3f}]")
        
        # Clasificar categor√≠as
        logger.info("\nüéØ CLASIFICACI√ìN:")
        
        logger.info("\n  EFECTO POLVOR√çN EXTREMO (Œ≤ > 2.0):")
        for cat, params in categorias_sorted:
            if params['beta'] > 2.0:
                logger.info(f"    ‚Ä¢ {cat}: Œ≤={params['beta']:.2f}")
        
        logger.info("\n  EFECTO POLVOR√çN ALTO (1.5 < Œ≤ ‚â§ 2.0):")
        for cat, params in categorias_sorted:
            if 1.5 < params['beta'] <= 2.0:
                logger.info(f"    ‚Ä¢ {cat}: Œ≤={params['beta']:.2f}")
        
        logger.info("\n  EFECTO POLVOR√çN MODERADO (1.0 < Œ≤ ‚â§ 1.5):")
        for cat, params in categorias_sorted:
            if 1.0 < params['beta'] <= 1.5:
                logger.info(f"    ‚Ä¢ {cat}: Œ≤={params['beta']:.2f}")
        
        logger.info("\n  EFECTO POLVOR√çN BAJO (Œ≤ ‚â§ 1.0):")
        for cat, params in categorias_sorted:
            if params['beta'] <= 1.0:
                logger.info(f"    ‚Ä¢ {cat}: Œ≤={params['beta']:.2f}")
    
    def predecir_con_contexto(self, noticia, categoria, vix_actual):
        """
        Predice usando par√°metros espec√≠ficos de la categor√≠a
        """
        # Buscar token
        token_data = self.df_tokens[
            (self.df_tokens['categoria'] == categoria) & 
            (self.df_tokens['asset'] == 'SPY')
        ]
        
        if len(token_data) == 0:
            p_base = 30.0
            token = 1.0
        else:
            token = token_data.iloc[0]['token']
            p_base = (token / 10.0) * 100
        
        # Usar par√°metros de la categor√≠a
        if categoria in self.params_por_categoria:
            params = self.params_por_categoria[categoria]
            alpha = params['alpha']
            beta = params['beta']
        else:
            # Par√°metros por defecto
            alpha = 0.75
            beta = 1.50
        
        # Calcular impacto contextual
        p_contextual = self.calcular_impacto_contextual(
            p_base,
            vix_actual,
            alpha,
            beta
        )
        
        v_norm = vix_actual / self.vix_critico
        ajuste = (p_contextual / p_base - 1) * 100 if p_base > 0 else 0
        
        return {
            'noticia': noticia,
            'categoria': categoria,
            'token': token,
            'probabilidad_base': p_base,
            'vix_actual': vix_actual,
            'vix_normalizado': v_norm,
            'probabilidad_contextual': p_contextual,
            'ajuste_por_vix': ajuste,
            'alpha': alpha,
            'beta': beta
        }
    
    def guardar_modelo(self):
        """Guarda modelo con par√°metros por categor√≠a"""
        timestamp = datetime.now().strftime('%Y%m%d')
        filepath = MODELS_DIR / f"modelo_refinado_vix_categorias_{timestamp}.pkl"
        
        modelo = {
            'params_por_categoria': self.params_por_categoria,
            'vix_critico': self.vix_critico,
            'df_tokens': self.df_tokens
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(modelo, f)
        
        logger.info(f"\n‚úì Modelo guardado: {filepath}")
        
        # Tambi√©n guardar JSON legible
        json_path = PROCESSED_DATA_DIR / "landau" / f"parametros_por_categoria_{timestamp}.json"
        with open(json_path, 'w') as f:
            json.dump(self.params_por_categoria, f, indent=2)
        
        logger.info(f"‚úì Par√°metros JSON: {json_path}")
        
        return filepath


def main():
    """Pipeline completo"""
    logger.info("="*70)
    logger.info("MODELO REFINADO VIX - POR CATEGOR√çA")
    logger.info("="*70)
    logger.info("Optimiza Œ± y Œ≤ espec√≠ficos para cada tipo de noticia")
    logger.info("")
    
    modelo = ModeloRefinadoVIXCategorias()
    
    # 1. Cargar datos
    logger.info("\n„ÄêFASE 1„Äë Cargando datos...")
    modelo.cargar_datos()
    
    # 2. Preparar datos por categor√≠a
    logger.info("\n„ÄêFASE 2„Äë Preparando datos por categor√≠a...")
    datos_por_categoria = modelo.preparar_datos_por_categoria()
    
    if len(datos_por_categoria) == 0:
        logger.error("\n‚ùå No hay datos suficientes")
        return
    
    # 3. Optimizar cada categor√≠a
    logger.info("\n„ÄêFASE 3„Äë Optimizaci√≥n Bayesiana...")
    modelo.optimizar_todas_categorias(datos_por_categoria)
    
    # 4. Analizar par√°metros
    logger.info("\n„ÄêFASE 4„Äë An√°lisis de par√°metros...")
    modelo.analizar_parametros()
    
    # 5. Guardar modelo
    logger.info("\n„ÄêFASE 5„Äë Guardando modelo...")
    modelo.guardar_modelo()
    
    # 6. Ejemplos
    logger.info("\n„ÄêFASE 6„Äë Ejemplos de predicci√≥n...")
    logger.info("="*70)
    
    ejemplos = [
        ("Terrorist attack in Europe", "terrorism", 15),
        ("Terrorist attack in Europe", "terrorism", 35),
        ("Fed raises rates", "fed_rates", 15),
        ("Fed raises rates", "fed_rates", 35),
        ("Housing sales increase", "us_housing", 15),
        ("Housing sales increase", "us_housing", 35),
    ]
    
    for noticia, cat, vix in ejemplos:
        pred = modelo.predecir_con_contexto(noticia, cat, vix)
        
        logger.info(f"\n'{noticia}'")
        logger.info(f"  VIX={vix}, Œ±={pred['alpha']:.2f}, Œ≤={pred['beta']:.2f}")
        logger.info(f"  P_base: {pred['probabilidad_base']:.1f}% ‚Üí P_ctx: {pred['probabilidad_contextual']:.1f}% "
                   f"({pred['ajuste_por_vix']:+.1f}%)")
    
    logger.info("\n" + "="*70)
    logger.info("‚úì‚úì‚úì MODELO POR CATEGOR√çAS COMPLETADO ‚úì‚úì‚úì")
    logger.info("="*70)


if __name__ == "__main__":
    main()

Modelo Refinado con VIX Contextual - Por Categor√≠a
Optimiza Œ± y Œ≤ espec√≠ficos para CADA tipo de noticia

HIP√ìTESIS:
- Noticias de guerra/terror: Œ≤ alto (efecto polvor√≠n extremo)
- Noticias Fed/ECB: Œ± y Œ≤ moderados
- Noticias housing/earnings: Œ± bajo (poco efecto VIX)
"""
import pandas as pd
import numpy as np
from datetime import datetime
import pickle
import sys
from pathlib import Path
import json

sys.path.append(str(Path(__file__).resolve().parent.parent.parent))

from src.utils.config import RAW_DATA_DIR, PROCESSED_DATA_DIR, MODELS_DIR
from src.utils.logger import logger


class ModeloRefinadoVIXCategorias:
    """
    Modelo que optimiza Œ± y Œ≤ por categor√≠a de noticia
    """
    
    def __init__(self):
        # Par√°metros por categor√≠a
        self.params_por_categoria = {}
        
        # VIX cr√≠tico
        self.vix_critico = 20.0
        
        # Datos
        self.df_tokens = None
        self.df_noticias = None
        self.df_historico = None
        
        logger.info("‚úì ModeloRefinadoVIXCategorias inicializado")
    
    def cargar_datos(self):
        """Carga todos los datos necesarios"""
        logger.info("\n" + "="*70)
        logger.info("CARGANDO DATOS")
        logger.info("="*70)
        
        # Tokens
        token_files = list(PROCESSED_DATA_DIR.glob("landau/tokens_volatilidad_*.csv"))
        if token_files:
            self.df_tokens = pd.read_csv(token_files[0])
            logger.info(f"  ‚úì Tokens: {len(self.df_tokens)} combinaciones")
        
        # Hist√≥rico
        hist_files = list(PROCESSED_DATA_DIR.glob("landau/parametros_landau_historicos_*.csv"))
        if hist_files:
            self.df_historico = pd.read_csv(hist_files[0])
            self.df_historico['fecha'] = pd.to_datetime(self.df_historico['fecha'])
            self.df_historico['fecha_norm'] = pd.to_datetime([str(d).split()[0] for d in self.df_historico['fecha']])
            logger.info(f"  ‚úì Hist√≥rico: {len(self.df_historico)} d√≠as")
        
        # Noticias
        kaggle_dir = RAW_DATA_DIR / "Kanggle"
        df_combined = pd.read_csv(kaggle_dir / "Combined_News_DJIA.csv")
        df_combined['Date'] = pd.to_datetime(df_combined['Date'])
        
        noticias = []
        for idx, row in df_combined.iterrows():
            for i in range(1, 26):
                noticia = row.get(f'Top{i}')
                if pd.notna(noticia) and noticia != '':
                    noticias.append({
                        'fecha': row['Date'],
                        'titulo': noticia
                    })
        
        self.df_noticias = pd.DataFrame(noticias)
        logger.info(f"  ‚úì Noticias: {len(self.df_noticias)}")
        
        self.clasificar_noticias_detallado()
        
        return True
    
    def clasificar_noticias_detallado(self):
        """Clasificaci√≥n detallada con m√∫ltiples categor√≠as"""
        categorias = {
            # ALTA VOLATILIDAD (esperamos Œ≤ alto)
            'terrorism': ['terror', 'bombing', 'attack', 'killed'],
            'war_russia': ['russia', 'ukraine', 'putin', 'kremlin'],
            'war_middle_east': ['iran', 'iraq', 'syria', 'israel', 'palestine'],
            'financial_crisis': ['crisis', 'crash', 'panic', 'bailout', 'collapse'],
            
            # MEDIA-ALTA VOLATILIDAD (esperamos Œ± y Œ≤ moderado-alto)
            'fed_rates': ['fed', 'fomc', 'interest rate', 'federal reserve'],
            'ecb_policy': ['ecb', 'draghi', 'lagarde', 'european central bank'],
            'oil_shock': ['oil price', 'opec', 'crude', 'petroleum'],
            
            # MEDIA VOLATILIDAD (esperamos Œ± y Œ≤ moderado)
            'us_gdp_data': ['gdp', 'economic growth', 'gross domestic'],
            'us_employment_data': ['employment', 'jobs', 'unemployment', 'payroll'],
            'china_economy': ['china', 'beijing', 'yuan', 'chinese'],
            
            # BAJA VOLATILIDAD (esperamos Œ± bajo, Œ≤ bajo)
            'us_housing': ['housing', 'home sales', 'real estate'],
            'corporate_earnings': ['earnings', 'profit', 'quarterly results'],
            'trade_data': ['trade', 'exports', 'imports'],
        }
        
        self.df_noticias['categoria'] = 'other'
        
        for idx, row in self.df_noticias.iterrows():
            titulo = str(row['titulo']).lower()
            for cat, keywords in categorias.items():
                if any(kw in titulo for kw in keywords):
                    self.df_noticias.at[idx, 'categoria'] = cat
                    break
        
        # Estad√≠sticas
        logger.info("\n  Distribuci√≥n por categor√≠a:")
        for cat in self.df_noticias['categoria'].value_counts().head(10).items():
            logger.info(f"    {cat[0]:<25s}: {cat[1]:>5,} noticias")
    
    def preparar_datos_por_categoria(self):
        """
        Prepara datasets separados por categor√≠a
        """
        logger.info("\n" + "="*70)
        logger.info("PREPARANDO DATOS POR CATEGOR√çA")
        logger.info("="*70)
        
        datos_por_categoria = {}
        
        categorias_validas = [c for c in self.df_noticias['categoria'].unique() if c != 'other']
        
        for categoria in categorias_validas:
            logger.info(f"\n  Procesando: {categoria}")
            
            df_cat = self.df_noticias[self.df_noticias['categoria'] == categoria]
            
            datos_opt = []
            
            # Buscar token
            token_data = self.df_tokens[
                (self.df_tokens['categoria'] == categoria) & 
                (self.df_tokens['asset'] == 'SPY')
            ]
            
            if len(token_data) == 0:
                logger.info(f"    ‚ö† Sin token, saltando")
                continue
            
            token = token_data.iloc[0]['token']
            p_base = (token / 10.0) * 100
            
            # Para cada noticia de esta categor√≠a
            for idx, row in df_cat.iterrows():
                fecha = pd.Timestamp(row['fecha']).normalize()
                
                # Buscar VIX y retorno
                hist_data = self.df_historico[self.df_historico['fecha_norm'] == fecha]
                
                if len(hist_data) == 0:
                    # Buscar en ventana
                    for offset in range(1, 4):
                        fecha_alt = fecha + pd.Timedelta(days=offset)
                        hist_data = self.df_historico[self.df_historico['fecha_norm'] == fecha_alt]
                        if len(hist_data) > 0:
                            break
                
                if len(hist_data) > 0:
                    vix_dia = hist_data.iloc[0]['vix']
                    retorno_real = hist_data.iloc[0]['sp500_return_1d']
                    
                    if pd.notna(vix_dia) and pd.notna(retorno_real):
                        impacto_real = 1 if abs(retorno_real) > 0.005 else 0
                        
                        datos_opt.append({
                            'p_base': p_base,
                            'vix': float(vix_dia),
                            'impacto_real': impacto_real,
                            'retorno_real': float(retorno_real)
                        })
            
            if len(datos_opt) >= 30:  # M√≠nimo 30 observaciones
                df_opt = pd.DataFrame(datos_opt)
                datos_por_categoria[categoria] = df_opt
                
                logger.info(f"    ‚úì {len(df_opt)} observaciones")
                logger.info(f"      Impacto (>0.5%): {df_opt['impacto_real'].mean()*100:.1f}%")
                logger.info(f"      VIX promedio: {df_opt['vix'].mean():.1f}")
            else:
                logger.info(f"    ‚ö† Solo {len(datos_opt)} obs, insuficiente")
        
        logger.info(f"\n‚úì Total categor√≠as con datos: {len(datos_por_categoria)}")
        
        return datos_por_categoria
    
    def calcular_impacto_contextual(self, p_base, vix, alpha, beta):
        """Calcula impacto con par√°metros espec√≠ficos"""
        v_norm = vix / self.vix_critico
        
        if v_norm <= 1.0:
            impacto = p_base * (1.0 - alpha * 0.1 * (1 - v_norm))
        else:
            impacto = p_base * (1.0 + alpha * ((v_norm - 1.0) ** beta))
        
        return max(0, min(100, impacto))
    
    def optimizar_categoria(self, categoria, df_cat):
        """
        Optimiza Œ± y Œ≤ para UNA categor√≠a espec√≠fica
        """
        logger.info(f"\n  Optimizando: {categoria}")
        
        try:
            from skopt import gp_minimize
            from skopt.space import Real
            from sklearn.metrics import f1_score
        except:
            logger.warning("    ‚ö† scikit-optimize no instalado, usando grid search")
            return self.optimizar_categoria_grid(categoria, df_cat)
        
        # Funci√≥n objetivo
        def objetivo(params):
            alpha, beta = params
            
            predictions = []
            for _, row in df_cat.iterrows():
                p_ctx = self.calcular_impacto_contextual(
                    row['p_base'],
                    row['vix'],
                    alpha,
                    beta
                )
                predictions.append(1 if p_ctx > 50 else 0)
            
            predictions = np.array(predictions)
            real = df_cat['impacto_real'].values
            
            # F1-score
            try:
                f1 = f1_score(real, predictions)
            except:
                f1 = 0.5
            
            return -f1
        
        # Espacio de b√∫squeda
        space = [
            Real(0.1, 2.5, name='alpha'),
            Real(0.5, 3.0, name='beta'),
        ]
        
        # Optimizar
        resultado = gp_minimize(
            objetivo,
            space,
            n_calls=30,  # 30 iteraciones por categor√≠a
            random_state=42,
            verbose=False
        )
        
        alpha_opt = resultado.x[0]
        beta_opt = resultado.x[1]
        f1_opt = -resultado.fun
        
        logger.info(f"    ‚úì Œ±={alpha_opt:.3f}, Œ≤={beta_opt:.3f}, F1={f1_opt:.3f}")
        
        return {
            'alpha': alpha_opt,
            'beta': beta_opt,
            'f1_score': f1_opt,
            'n_obs': len(df_cat)
        }
    
    def optimizar_categoria_grid(self, categoria, df_cat):
        """Grid search como fallback"""
        from sklearn.metrics import f1_score
        
        mejor_f1 = 0
        mejor_params = {'alpha': 0.75, 'beta': 1.5}
        
        for alpha in [0.3, 0.5, 0.75, 1.0, 1.5]:
            for beta in [0.8, 1.0, 1.2, 1.5, 2.0]:
                predictions = []
                for _, row in df_cat.iterrows():
                    p_ctx = self.calcular_impacto_contextual(
                        row['p_base'],
                        row['vix'],
                        alpha,
                        beta
                    )
                    predictions.append(1 if p_ctx > 50 else 0)
                
                predictions = np.array(predictions)
                real = df_cat['impacto_real'].values
                
                try:
                    f1 = f1_score(real, predictions)
                    if f1 > mejor_f1:
                        mejor_f1 = f1
                        mejor_params = {'alpha': alpha, 'beta': beta}
                except:
                    pass
        
        logger.info(f"    ‚úì Œ±={mejor_params['alpha']:.3f}, Œ≤={mejor_params['beta']:.3f}, F1={mejor_f1:.3f}")
        
        return {
            'alpha': mejor_params['alpha'],
            'beta': mejor_params['beta'],
            'f1_score': mejor_f1,
            'n_obs': len(df_cat)
        }
    
    def optimizar_todas_categorias(self, datos_por_categoria):
        """
        Optimiza Œ± y Œ≤ para todas las categor√≠as
        """
        logger.info("\n" + "="*70)
        logger.info("OPTIMIZACI√ìN BAYESIANA POR CATEGOR√çA")
        logger.info("="*70)
        
        for categoria, df_cat in datos_por_categoria.items():
            params = self.optimizar_categoria(categoria, df_cat)
            self.params_por_categoria[categoria] = params
        
        logger.info("\n‚úì Optimizaci√≥n completada")
        logger.info(f"  Total categor√≠as: {len(self.params_por_categoria)}")
    
    def analizar_parametros(self):
        """
        Analiza los par√°metros encontrados por tipo de noticia
        """
        logger.info("\n" + "="*70)
        logger.info("AN√ÅLISIS DE PAR√ÅMETROS POR CATEGOR√çA")
        logger.info("="*70)
        
        # Ordenar por Œ≤ (efecto polvor√≠n)
        categorias_sorted = sorted(
            self.params_por_categoria.items(),
            key=lambda x: x[1]['beta'],
            reverse=True
        )
        
        logger.info("\nüî• RANKING POR EFECTO POLVOR√çN (Œ≤):")
        logger.info("‚îÄ"*70)
        logger.info(f"{'Categor√≠a':<25s} | {'Œ±':>6s} | {'Œ≤':>6s} | {'F1':>6s} | {'Obs':>6s}")
        logger.info("‚îÄ"*70)
        
        for cat, params in categorias_sorted:
            logger.info(f"{cat:<25s} | {params['alpha']:>6.3f} | {params['beta']:>6.3f} | "
                       f"{params['f1_score']:>6.3f} | {params['n_obs']:>6,}")
        
        # Estad√≠sticas
        betas = [p['beta'] for p in self.params_por_categoria.values()]
        alphas = [p['alpha'] for p in self.params_por_categoria.values()]
        
        logger.info("\nüìä ESTAD√çSTICAS:")
        logger.info(f"  Œ≤ promedio: {np.mean(betas):.3f}")
        logger.info(f"  Œ≤ min-max: [{np.min(betas):.3f}, {np.max(betas):.3f}]")
        logger.info(f"  Œ± promedio: {np.mean(alphas):.3f}")
        logger.info(f"  Œ± min-max: [{np.min(alphas):.3f}, {np.max(alphas):.3f}]")
        
        # Clasificar categor√≠as
        logger.info("\nüéØ CLASIFICACI√ìN:")
        
        logger.info("\n  EFECTO POLVOR√çN EXTREMO (Œ≤ > 2.0):")
        for cat, params in categorias_sorted:
            if params['beta'] > 2.0:
                logger.info(f"    ‚Ä¢ {cat}: Œ≤={params['beta']:.2f}")
        
        logger.info("\n  EFECTO POLVOR√çN ALTO (1.5 < Œ≤ ‚â§ 2.0):")
        for cat, params in categorias_sorted:
            if 1.5 < params['beta'] <= 2.0:
                logger.info(f"    ‚Ä¢ {cat}: Œ≤={params['beta']:.2f}")
        
        logger.info("\n  EFECTO POLVOR√çN MODERADO (1.0 < Œ≤ ‚â§ 1.5):")
        for cat, params in categorias_sorted:
            if 1.0 < params['beta'] <= 1.5:
                logger.info(f"    ‚Ä¢ {cat}: Œ≤={params['beta']:.2f}")
        
        logger.info("\n  EFECTO POLVOR√çN BAJO (Œ≤ ‚â§ 1.0):")
        for cat, params in categorias_sorted:
            if params['beta'] <= 1.0:
                logger.info(f"    ‚Ä¢ {cat}: Œ≤={params['beta']:.2f}")
    
    def predecir_con_contexto(self, noticia, categoria, vix_actual):
        """
        Predice usando par√°metros espec√≠ficos de la categor√≠a
        """
        # Buscar token
        token_data = self.df_tokens[
            (self.df_tokens['categoria'] == categoria) & 
            (self.df_tokens['asset'] == 'SPY')
        ]
        
        if len(token_data) == 0:
            p_base = 30.0
            token = 1.0
        else:
            token = token_data.iloc[0]['token']
            p_base = (token / 10.0) * 100
        
        # Usar par√°metros de la categor√≠a
        if categoria in self.params_por_categoria:
            params = self.params_por_categoria[categoria]
            alpha = params['alpha']
            beta = params['beta']
        else:
            # Par√°metros por defecto
            alpha = 0.75
            beta = 1.50
        
        # Calcular impacto contextual
        p_contextual = self.calcular_impacto_contextual(
            p_base,
            vix_actual,
            alpha,
            beta
        )
        
        v_norm = vix_actual / self.vix_critico
        ajuste = (p_contextual / p_base - 1) * 100 if p_base > 0 else 0
        
        return {
            'noticia': noticia,
            'categoria': categoria,
            'token': token,
            'probabilidad_base': p_base,
            'vix_actual': vix_actual,
            'vix_normalizado': v_norm,
            'probabilidad_contextual': p_contextual,
            'ajuste_por_vix': ajuste,
            'alpha': alpha,
            'beta': beta
        }
    
    def guardar_modelo(self):
        """Guarda modelo con par√°metros por categor√≠a"""
        timestamp = datetime.now().strftime('%Y%m%d')
        filepath = MODELS_DIR / f"modelo_refinado_vix_categorias_{timestamp}.pkl"
        
        modelo = {
            'params_por_categoria': self.params_por_categoria,
            'vix_critico': self.vix_critico,
            'df_tokens': self.df_tokens
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(modelo, f)
        
        logger.info(f"\n‚úì Modelo guardado: {filepath}")
        
        # Tambi√©n guardar JSON legible
        json_path = PROCESSED_DATA_DIR / "landau" / f"parametros_por_categoria_{timestamp}.json"
        with open(json_path, 'w') as f:
            json.dump(self.params_por_categoria, f, indent=2)
        
        logger.info(f"‚úì Par√°metros JSON: {json_path}")
        
        return filepath


def main():
    """Pipeline completo"""
    logger.info("="*70)
    logger.info("MODELO REFINADO VIX - POR CATEGOR√çA")
    logger.info("="*70)
    logger.info("Optimiza Œ± y Œ≤ espec√≠ficos para cada tipo de noticia")
    logger.info("")
    
    modelo = ModeloRefinadoVIXCategorias()
    
    # 1. Cargar datos
    logger.info("\n„ÄêFASE 1„Äë Cargando datos...")
    modelo.cargar_datos()
    
    # 2. Preparar datos por categor√≠a
    logger.info("\n„ÄêFASE 2„Äë Preparando datos por categor√≠a...")
    datos_por_categoria = modelo.preparar_datos_por_categoria()
    
    if len(datos_por_categoria) == 0:
        logger.error("\n‚ùå No hay datos suficientes")
        return
    
    # 3. Optimizar cada categor√≠a
    logger.info("\n„ÄêFASE 3„Äë Optimizaci√≥n Bayesiana...")
    modelo.optimizar_todas_categorias(datos_por_categoria)
    
    # 4. Analizar par√°metros
    logger.info("\n„ÄêFASE 4„Äë An√°lisis de par√°metros...")
    modelo.analizar_parametros()
    
    # 5. Guardar modelo
    logger.info("\n„ÄêFASE 5„Äë Guardando modelo...")
    modelo.guardar_modelo()
    
    # 6. Ejemplos
    logger.info("\n„ÄêFASE 6„Äë Ejemplos de predicci√≥n...")
    logger.info("="*70)
    
    ejemplos = [
        ("Terrorist attack in Europe", "terrorism", 15),
        ("Terrorist attack in Europe", "terrorism", 35),
        ("Fed raises rates", "fed_rates", 15),
        ("Fed raises rates", "fed_rates", 35),
        ("Housing sales increase", "us_housing", 15),
        ("Housing sales increase", "us_housing", 35),
    ]
    
    for noticia, cat, vix in ejemplos:
        pred = modelo.predecir_con_contexto(noticia, cat, vix)
        
        logger.info(f"\n'{noticia}'")
        logger.info(f"  VIX={vix}, Œ±={pred['alpha']:.2f}, Œ≤={pred['beta']:.2f}")
        logger.info(f"  P_base: {pred['probabilidad_base']:.1f}% ‚Üí P_ctx: {pred['probabilidad_contextual']:.1f}% "
                   f"({pred['ajuste_por_vix']:+.1f}%)")
    
    logger.info("\n" + "="*70)
    logger.info("‚úì‚úì‚úì MODELO POR CATEGOR√çAS COMPLETADO ‚úì‚úì‚úì")
    logger.info("="*70)


if __name__ == "__main__":
    main()



